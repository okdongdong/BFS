{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce57961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import sleep\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import ElementNotInteractableException\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1f6175",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_list = ['중구 중앙동', '중구 동광동', '중구 대청동', '중구 보수동', '중구 부평동',\n",
    "          '중구 광복동', '중구 남포동', '중구 영주동', '서구 동대신동', '서구 서대신동',\n",
    "          '서구 부민동', '서구 아미동', '서구 초장동', '서구 충무동', '서구 남부민동',\n",
    "          '서구 암남동', '동구 초량동', '동구 수정동', '동구 좌천동', '동구 범일동',\n",
    "          '영도구 남항동', '영도구 영선동', '영도구 신선동', '영도구 봉래동', '영도구 청학동',\n",
    "          '영도구 동삼동', '부산진구 부전동', '부산진구 연지동', '부산진구 초읍동',\n",
    "          '부산진구 양정동', '부산진구 전포동', '부산진구 부암동', '부산진구 당감동',\n",
    "          '부산진구 가야동', '부산진구 개금동', '부산진구 범천동', '동래구 수민동',\n",
    "          '동래구 복산동', '동래구 명륜동', '동래구 온천동', '동래구 사직동', '동래구 안락동',\n",
    "          '동래구 명장동', '남구 대연동', '남구 용호동', '남구 용당동', '남구 감만동', '남구 우암동',\n",
    "          '남구 문현동', '북구 구포동', '북구 금곡동', '북구 화명동', '북구 덕천동', '북구 만덕동',\n",
    "          '해운대구 우동', '해운대구 중동', '해운대구 좌동', '해운대구 송정동', '해운대구 반여동',\n",
    "          '해운대구 반송동', '해운대구 재송동', '사하구 괴정동', '사하구 당리동', '사하구 감천동',\n",
    "          '사하구 다대동', '사하구 구평동', '사하구 장림동', '사하구 신평동', '사하구 하단동',\n",
    "          '금정구 서동', '금정구 금사동', '금정구 부곡동', '금정구 장전동', '금정구 남산동', '금정구 구서동',\n",
    "          '금정구 청룡동', '금정구 선동', '금정구 금성동', '강서구 대저동', '강서구 강동동',\n",
    "          '강서구 명지동', '강서구 가락동', '강서구 녹산동', '강서구 천성동', '연제구 거제동',\n",
    "          '연제구  연산동', '수영구 남천동', '수영구 수영동', '수영구 망미동', '수영구 광안동', '수영구 민락동',\n",
    "          '사상구 삼락동', '사상구 모라동', '사상구 덕포동', '사상구 쾌법동', '사상구 감전동', '사상구 주례동', '사상구 학장동', '사상구 엄궁동',\n",
    "          '기장군 기장읍', '기장군 장안읍', '기장군 정관읍', '기장군 일광면', '기장군 철마면']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e2aa5",
   "metadata": {},
   "source": [
    "- 중구 보수동 해야함\n",
    "- 부평동 분식까지함\n",
    "- 광복동 분식까지함\n",
    "- 중구 남포동 한식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da0bca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorys = ['간식', '분식', '뷔페', '술집', '아시아음식', '양식', '일식', '중식', '패스트푸드', '패밀리레스토랑', '피자', '치킨', '한식', '카페']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f87e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "gu_list = ['중구 남포동', '중구 영주동', '서구 동대신동', '서구 서대신동',\n",
    "          '서구 부민동', '서구 아미동', '서구 초장동', '서구 충무동', '서구 남부민동',\n",
    "          '서구 암남동', '동구 초량동', '동구 수정동', '동구 좌천동', '동구 범일동',\n",
    "          '영도구 남항동', '영도구 영선동', '영도구 신선동', '영도구 봉래동', '영도구 청학동',\n",
    "          '영도구 동삼동', '부산진구 부전동', '부산진구 연지동', '부산진구 초읍동',\n",
    "          '부산진구 양정동', '부산진구 전포동', '부산진구 부암동', '부산진구 당감동',\n",
    "          '부산진구 가야동', '부산진구 개금동', '부산진구 범천동', '동래구 수민동',\n",
    "          '동래구 복산동', '동래구 명륜동', '동래구 온천동', '동래구 사직동', '동래구 안락동',\n",
    "          '동래구 명장동', '남구 대연동', '남구 용호동', '남구 용당동', '남구 감만동', '남구 우암동',\n",
    "          '남구 문현동', '북구 구포동', '북구 금곡동', '북구 화명동', '북구 덕천동', '북구 만덕동',\n",
    "          '해운대구 우동', '해운대구 중동', '해운대구 좌동', '해운대구 송정동', '해운대구 반여동',\n",
    "          '해운대구 반송동', '해운대구 재송동', '사하구 괴정동', '사하구 당리동', '사하구 감천동',\n",
    "          '사하구 다대동', '사하구 구평동', '사하구 장림동', '사하구 신평동', '사하구 하단동',\n",
    "          '금정구 서동', '금정구 금사동', '금정구 부곡동', '금정구 장전동', '금정구 남산동', '금정구 구서동',\n",
    "          '금정구 청룡동', '금정구 선동', '금정구 금성동', '강서구 대저동', '강서구 강동동',\n",
    "          '강서구 명지동', '강서구 가락동', '강서구 녹산동', '강서구 천성동', '연제구 거제동',\n",
    "          '연제구  연산동', '수영구 남천동', '수영구 수영동', '수영구 망미동', '수영구 광안동', '수영구 민락동',\n",
    "          '사상구 삼락동', '사상구 모라동', '사상구 덕포동', '사상구 쾌법동', '사상구 감전동', '사상구 주례동', '사상구 학장동', '사상구 엄궁동',\n",
    "          '기장군 기장읍', '기장군 장안읍', '기장군 정관읍', '기장군 일광면', '기장군 철마면']\n",
    "categorys = ['카페']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b742694",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------중구 남포동카페--------------------\n",
      "채도 1\n",
      "투썸플레이스 부산남포점 2\n",
      "탑플로어 3\n",
      "클라우드밤 4\n",
      "스타벅스 자갈치역점 5\n",
      "스타벅스 비프광장점 6\n",
      "뚜벅스 남포점 7\n",
      "꿈꾸는다락방 8\n",
      "펫카페라라랜드 9\n",
      "미스터힐링 남포동점 10\n",
      "네루다커피 11\n",
      "카페슬로 12\n",
      "코드네임블랙 부산점 13\n",
      "커핑브레드 남포본점 14\n",
      "할리스 부산BIFF광장점 15\n",
      "아수라발발타 남포점 16\n",
      "이디야커피 부산광복중앙점 17\n",
      "빽다방 부산광복로점 18\n",
      "탐앤탐스 부산남포점 19\n",
      "서정다방 20\n",
      "하삼동커피 남포점 21\n",
      "고망고 남포점 22\n",
      "영커피 남포점 23\n",
      "양이양이 24\n",
      "카페부스 25\n",
      "카페반디나무 26\n",
      "아마스빈 남포점 27\n",
      "공차 부산남포점 28\n",
      "심신프리 부산점 29\n",
      "오소아 비프점 30\n",
      "더타임나우 31\n",
      "매머드익스프레스 부산남포점 32\n",
      "컴포즈커피 BIFF거리점 33\n",
      "커피에반하다 부산광복로데오점 34\n",
      "팔공티 부산남포점 35\n",
      "더리터 비프광장점 36\n",
      "다해정 37\n",
      "차앤 38\n",
      "모히또바인오션 롯데시네마 대영점 39\n",
      "카페밈 40\n",
      "경복커피숖 41\n",
      "카페051 남포역점 42\n",
      "꿈꾸는다락방 2호점 43\n",
      "신한커피숍 44\n",
      "부일커피숍 45\n",
      "커피민 46\n",
      "바니샌드 47\n",
      "호수커피숍 48\n",
      "자유공간 49\n",
      "남도커피숖 50\n",
      "홀릭에스프레소 51\n",
      "카페라비아 52\n",
      "당근나라 53\n",
      "탑스빈 KEB하나은행점 54\n",
      "레인보우라이브 55\n",
      "예지원 56\n",
      "고구마명가 57\n",
      "러브보트 58\n",
      "쉼표셀프힐링카페 59\n",
      "티맑은 남포점 60\n",
      "잇커피 자갈치점 61\n",
      "커피루틴 비프광장점 62\n",
      "북카페 63\n",
      "얼리지언스 64\n",
      "카페지와이 65\n",
      "아그리나 드레스체험카페 66\n",
      "마이쥬스 남포점 67\n",
      "도피오 68\n",
      "잣나무 69\n",
      "통카페 70\n",
      "라무 71\n",
      "타임커피 72\n",
      "세시봉 73\n",
      "블랙레이디 74\n",
      "까까로운 75\n",
      "마피아쥬스 남포동점 76\n",
      "커피반해 77\n",
      "더테이블 78\n",
      "굿마우스필 79\n",
      "부산컨세션 80\n",
      "카페가비 81\n",
      "남포당 82\n",
      "아망떼룸카페 83\n",
      "쿠오리노 84\n",
      "스타벅스 부산광복로R점 85\n",
      "아인스크레페 86\n",
      "투썸플레이스 부산광복점 87\n",
      "공차 부산광복점 88\n",
      "와플칸 89\n",
      "스타벅스 남포동역점 90\n",
      "핑크게이트 91\n",
      "덕수네애견카페 광복로점 92\n",
      "설빙 부산남포점 93\n",
      "어피치카페 94\n",
      "룸즈에이 부산점 95\n",
      "광복동 12시 96\n",
      "쿠키어클락 97\n",
      "이디야커피 부산광복로점 98\n",
      "그라운드27 99\n",
      "설빙 광복점 100\n",
      "꽃바테 101\n",
      "고양이다락방 부산남포점 102\n",
      "릴라이 103\n",
      "레드버튼 남포점 104\n",
      "어벤더치커피 남포점 105\n",
      "도깨비 남포점 106\n",
      "더벤티 광복점 107\n",
      "커피빈 부산광복동점 108\n",
      "까사오로 109\n",
      "레귤러하우스 남포점 110\n",
      "신창동커피 111\n",
      "컴포즈커피 광복점 112\n",
      "카페광복동 113\n",
      "바우노바커피 114\n",
      "여여해 115\n",
      "랑데자뷰 남포점 116\n",
      "부산이스케이프 남포2호점 117\n",
      "하삼동커피 광복점 118\n",
      "커피빈 부산남포역점 119\n",
      "카페베네 부산남포점 120\n",
      "오드이븐 121\n",
      "커피 122\n",
      "카페1964 123\n",
      "남포사주&타로 124\n",
      "매일이 다르다 125\n",
      "비키룸 126\n",
      "나담 127\n",
      "슬라임월드 128\n",
      "식후빵 129\n",
      "라임스케일 130\n",
      "블루샥 대청동점 131\n",
      "디저트39 부산남포점 132\n",
      "엔제리너스 부산남포점 133\n",
      "삼진커피 134\n",
      "네이처가든카페 135\n",
      "어벤더치커피 광복점 136\n",
      "음악이좋은사람들 137\n",
      "에쎄떼 138\n",
      "게네랄파우제 139\n",
      "제이제이인커피 140\n",
      "신디시스 141\n",
      "젠틀러커피 142\n",
      "JJ in coffee 143\n",
      "두달팽이 144\n",
      "제노아 145\n",
      "카페67st 146\n",
      "그리다부부 147\n",
      "다산 148\n",
      "볼피 149\n",
      "화방다방 150\n",
      "양다방 151\n",
      "텐퍼센트커피 부산동광동점 152\n",
      "커피공방 153\n",
      "커피후 광복로점 154\n",
      "폴바셋 롯데백화점광복점 155\n",
      "빽다방 부산남포광장점 156\n",
      "렛미아웃 남포점 157\n",
      "황가팥빙수단팥죽 158\n",
      "르 쁘티갸또 159\n",
      "화이트캐슬 남포점 160\n",
      "바모노스 161\n",
      "바우노바백산 162\n",
      "베란다 163\n",
      "마이데이 164\n",
      "스타벅스 영도대교점 165\n",
      "아나브린 166\n",
      "닥터밸런스 광복점 167\n",
      "슈가팩토리 168\n",
      "카페드파리 롯데백화점광복점 169\n",
      "핑크코끼리 광복점 170\n",
      "엘 171\n",
      "체크인부산 172\n",
      "카페이네이티드 173\n",
      "굿올데이즈카페 174\n",
      "이디야커피 동광동점 175\n",
      "탐스커버리 중앙점 176\n",
      "수림카페 177\n",
      "룸까페 178\n",
      "쟈스민커피 179\n",
      "차랑 180\n",
      "77다방 181\n",
      "공차 롯데마트광복점 182\n",
      "카페아슬란 롯데광복점 183\n",
      "목커피 184\n",
      "커먼피플 185\n",
      "리프패럿 롯데마트 광복점 186\n",
      "어반그린 187\n",
      "어벤더치커피 부평동점 188\n",
      "가배공방 189\n",
      "주아스커피 190\n",
      "루트커피 191\n",
      "9duck 192\n",
      "연경재 193\n",
      "브라운핸즈 롯데백화점 광복점 194\n",
      "NGU 앤 아이리얼파크 롯데백화점광복점 195\n",
      "엘라보라도 196\n",
      "후후랩 197\n",
      "줄리스카페 198\n",
      "광복동커피 199\n",
      "크레페마고 200\n",
      "메가커피 광복2호점 201\n",
      "플랜지 202\n",
      "아지트 203\n",
      "카페990 204\n",
      "살라비 205\n",
      "일리카페 롯데백화점광복점 206\n",
      "라임 207\n",
      "노티스 208\n",
      "디엘프렌즈 롯데백화점 광복점 209\n",
      "유나커피 210\n",
      "얌이밀 롯데광복점 211\n",
      "페르소나 212\n",
      "쥬씨 부산부평점 213\n",
      "골목안작은봄날 214\n",
      "체스커피전문점 215\n",
      "메가커피 서구청점 216\n",
      "신나는마녀 217\n",
      "그린게이트 218\n",
      "이디야 부산동광동점 219\n",
      "깡통시장바리스타PTG 220\n",
      "깡통커피 221\n",
      "킬러스웰 222\n",
      "프라미스랜드 223\n",
      "커피통 224\n",
      "커피볶는바리스타 225\n",
      "키작은여자카페 226\n",
      "더리터 부평점 227\n",
      "제주스 롯데백화점 광복점 228\n",
      "스타벅스 국제시장점 229\n",
      "패스더브라운 230\n",
      "커피트리스 광복점 231\n",
      "에스와이씨 232\n",
      "투게더 233\n",
      "컵테이블 234\n",
      "화음꽃차랑후식 235\n",
      "쓰리게이트 236\n",
      "라비타커피 237\n",
      "본아베띠 롯데백화점 광복점 238\n",
      "카페051 부평점 239\n",
      "투썸플레이스 부산대한통운점 240\n",
      "칸지 241\n",
      "카페그린듯 242\n",
      "행복한아침 243\n",
      "드립커피 244\n",
      "섬다방 245\n",
      "아리스타커피 부산가로수길점 246\n",
      "커피트리 247\n",
      "카페말포이 248\n",
      "눈카페 249\n",
      "레드아이커피 250\n",
      "땡큐메리머치 251\n",
      "카페앨리스 252\n",
      "써니빌커피하우스 253\n",
      "삼오 254\n",
      "로얄다방 255\n",
      "깡깡이카페 256\n",
      "라발스스카이카페&바 257\n",
      "일첼로 부산중구지역자활센터 258\n",
      "플루800 부평시장점 259\n",
      "수다방 260\n",
      "아우라엔틱카페 261\n",
      "마미스카페앤젤라또 262\n",
      "원더번스 남포점 263\n",
      "하삼동커피 자갈치역점 264\n",
      "뉴성남관광호텔커피숍 265\n",
      "우리커피숍 266\n",
      "제니 267\n",
      "케이팝 라이브 하우스 268\n",
      "좋은차 269\n",
      "지앤에이치광복점 270\n",
      "이데아 271\n",
      "더프레쉬카페 272\n",
      "행운커피숍 273\n",
      "로타리커피숍 274\n",
      "인앤빈 보수점 275\n",
      "이디야커피 부산만세365병원점 276\n",
      "루틴 277\n",
      "수커피숍 278\n",
      "작설원 279\n",
      "차생원 부산점 280\n",
      "브니엘의아침 281\n"
     ]
    },
    {
     "ename": "UnexpectedAlertPresentException",
     "evalue": "Alert Text: 접속이 원활하지 않습니다.\n잠시 후 다시 이용해주세요.\nMessage: unexpected alert open: {Alert text : 접속이 원활하지 않습니다.\n잠시 후 다시 이용해주세요.}\n  (Session info: chrome=99.0.4844.82)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x005E69A3+2582947]\n\tOrdinal0 [0x0057A6D1+2139857]\n\tOrdinal0 [0x00473A98+1063576]\n\tOrdinal0 [0x004C818B+1409419]\n\tOrdinal0 [0x004B8366+1344358]\n\tOrdinal0 [0x00495176+1200502]\n\tOrdinal0 [0x00496066+1204326]\n\tGetHandleVerifier [0x0078BE02+1675858]\n\tGetHandleVerifier [0x0084036C+2414524]\n\tGetHandleVerifier [0x0067BB01+560977]\n\tGetHandleVerifier [0x0067A8D3+556323]\n\tOrdinal0 [0x0058020E+2163214]\n\tOrdinal0 [0x00585078+2183288]\n\tOrdinal0 [0x005851C0+2183616]\n\tOrdinal0 [0x0058EE1C+2223644]\n\tBaseThreadInitThunk [0x75BDFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77D37A7E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77D37A4E+238]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnexpectedAlertPresentException\u001b[0m           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2180\\3105305956.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m                                 \u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m                                 \u001b[1;31m# 스크롤 다운 후 스크롤 높이 다시 가져옴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m                                 \u001b[0mnew_height\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return document.body.scrollHeight\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mnew_height\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlast_height\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute_script\u001b[1;34m(self, script, *args)\u001b[0m\n\u001b[0;32m    879\u001b[0m         return self.execute(command, {\n\u001b[0;32m    880\u001b[0m             \u001b[1;34m'script'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m             'args': converted_args})['value']\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute_async_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscript\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    427\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32m~\\anaconda3\\envs\\test\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[1;34m'alert'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnexpectedAlertPresentException\u001b[0m: Alert Text: 접속이 원활하지 않습니다.\n잠시 후 다시 이용해주세요.\nMessage: unexpected alert open: {Alert text : 접속이 원활하지 않습니다.\n잠시 후 다시 이용해주세요.}\n  (Session info: chrome=99.0.4844.82)\nStacktrace:\nBacktrace:\n\tOrdinal0 [0x005E69A3+2582947]\n\tOrdinal0 [0x0057A6D1+2139857]\n\tOrdinal0 [0x00473A98+1063576]\n\tOrdinal0 [0x004C818B+1409419]\n\tOrdinal0 [0x004B8366+1344358]\n\tOrdinal0 [0x00495176+1200502]\n\tOrdinal0 [0x00496066+1204326]\n\tGetHandleVerifier [0x0078BE02+1675858]\n\tGetHandleVerifier [0x0084036C+2414524]\n\tGetHandleVerifier [0x0067BB01+560977]\n\tGetHandleVerifier [0x0067A8D3+556323]\n\tOrdinal0 [0x0058020E+2163214]\n\tOrdinal0 [0x00585078+2183288]\n\tOrdinal0 [0x005851C0+2183616]\n\tOrdinal0 [0x0058EE1C+2223644]\n\tBaseThreadInitThunk [0x75BDFA29+25]\n\tRtlGetAppContainerNamedObjectPath [0x77D37A7E+286]\n\tRtlGetAppContainerNamedObjectPath [0x77D37A4E+238]\n"
     ]
    }
   ],
   "source": [
    "# gu_list = ['중구 중앙동']\n",
    "# categorys = ['패밀리레스토랑', '패스트푸드']\n",
    "\n",
    "columns = ['name', 'category', 'address', 'tel', 'time', 'menu', 'image']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "review_columns = ['name', 'user_id', 'score', 'review']\n",
    "df_review = pd.DataFrame(columns=review_columns)\n",
    "\n",
    "for index, gu_name in enumerate(gu_list):\n",
    "    for category in categorys:\n",
    "        print(\"------------\" + gu_name + category + \"--------------------\")\n",
    "        time.sleep(random.randint(1, 10))\n",
    "        check = False\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('lang=ko_KR')\n",
    "        options.add_argument(\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36\")\n",
    "        options.add_argument('headless')\n",
    "        driver = webdriver.Chrome('./chromedriver')  # chromedriver 열기\n",
    "        driver.get('https://map.kakao.com/')\n",
    "        search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]') # 검색 창\n",
    "        search_area.send_keys('부산 '+ gu_name + ' ' + category)\n",
    "        driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)  # Enter로 검색\n",
    "        driver.implicitly_wait(1) # 기다려 주자\n",
    "        more_page = driver.find_element_by_id(\"info.search.place.more\")\n",
    "        \n",
    "        try:\n",
    "            more_page.send_keys(Keys.ENTER)\n",
    "            check = True\n",
    "#             print(\"눌렀다\")\n",
    "        except:\n",
    "#             print(\"못눌렀다\")\n",
    "            pass\n",
    "#         more_page.send_keys(Keys.ENTER) # 더보기 누르고\n",
    "#         print(more_page.get_attribute(\"class\").split(' '))\n",
    "        \n",
    "#         # 장소더보기 비활성화일때\n",
    "#         print(\"보자\",more_page.get_attribute(\"class\"))\n",
    "        \n",
    "        # 더보기를 누를 수 있으면\n",
    "        if check == True:\n",
    "#         if len(more_page.get_attribute(\"class\").split(' ')) > 1:\n",
    "#             more_page.send_keys(Keys.ENTER) # 더보기 누르고\n",
    "            time.sleep(random.randint(1, 3))\n",
    "        \n",
    "            # 다음 버튼 찾기\n",
    "            next_btn = driver.find_element_by_id(\"info.search.page.next\")\n",
    "#             print(\"이건?\", next_btn.get_attribute('class').split(' '))\n",
    "            has_next = \"disabled\" not in next_btn.get_attribute(\"class\").split(\" \")\n",
    "            Page = 1\n",
    "\n",
    "            # 5페이지가 넘을 때\n",
    "            if has_next:\n",
    "                while has_next: # 다음 페이지가 있으면 loop\n",
    "                    driver.implicitly_wait(1) # 기다려 주자\n",
    "                    page_links = driver.find_elements_by_css_selector(\"#info\\.search\\.page a\")\n",
    "                    pages = [link for link in page_links if \"HIDDEN\" not in link.get_attribute(\"class\").split(\" \")]\n",
    "        #             print(pages)\n",
    "                    for i in range(1, 6):\n",
    "                        xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "                        try:\n",
    "                            page = driver.find_element_by_xpath(xPath)\n",
    "                            page.send_keys(Keys.ENTER)\n",
    "                        except ElementNotInteractableException:\n",
    "#                             print('End of Page')\n",
    "                            break;\n",
    "                        time.sleep(random.randint(1, 10))\n",
    "                        place_lists = driver.find_elements_by_css_selector('#info\\.search\\.place\\.list > li')\n",
    "                        for p in place_lists: # WebElement\n",
    "                            store_html = p.get_attribute('innerHTML')\n",
    "                            store_info = BeautifulSoup(store_html, \"html.parser\")\n",
    "                            place_name = store_info.select('.head_item > .tit_name > .link_name')\n",
    "                            if len(place_name) == 0:\n",
    "                                continue # 광고\n",
    "\n",
    "                            place_name = store_info.select('.head_item > .tit_name > .link_name')[0].text\n",
    "                            place_address = store_info.select('.info_item > .addr > p')[0].text\n",
    "                            place_hour = store_info.select('.info_item > .openhour > p > a')[0].text\n",
    "                            place_tel = store_info.select('.info_item > .contact > span')[0].text\n",
    "                            place_category = store_info.select('.head_item > span')[0].text\n",
    "\n",
    "                            time.sleep(random.randint(1, 10))\n",
    "                            detail = p.find_element_by_css_selector('div.info_item > div.contact > a.moreview')\n",
    "                            detail.send_keys(Keys.ENTER)\n",
    "\n",
    "                            driver.switch_to.window(driver.window_handles[-1])\n",
    "                            driver.implicitly_wait(1) # 기다려 주자\n",
    "\n",
    "                            menu_lst = []\n",
    "\n",
    "                            last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                            while True:\n",
    "                                # 스크롤 끝까지 내리기\n",
    "                                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                                sleep(2)\n",
    "                                # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
    "                                new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                                if new_height == last_height:\n",
    "                                    break\n",
    "                                last_height = new_height\n",
    "\n",
    "                            html = driver.page_source\n",
    "                            soup = BeautifulSoup(html, 'html.parser')\n",
    "                            menu_all = soup.select('.cont_menu > .list_menu > .nophoto_type')\n",
    "                            menu_only_all = soup.select('.cont_menu > .list_menu > .menuonly_type')\n",
    "                            phototype = soup.select('.cont_menu > .list_menu > .photo_type')\n",
    "        #                     print(menu_all)\n",
    "                            if len(menu_all) != 0:\n",
    "                                for menu in menu_all:\n",
    "                                    menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                                    menuprices = menu.select('.info_menu > .price_menu')\n",
    "                                    menuprice = ''\n",
    "\n",
    "                                    if len(menuprices) != 0:\n",
    "                                        menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                                    menu_lst.append((menuname, menuprice))\n",
    "                            elif len(menu_only_all) != 0:\n",
    "                                for menu in menu_only_all:\n",
    "                                    menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                                    menuprices = menu.select('.info_menu > .price_menu')\n",
    "                                    menuprice = ''\n",
    "\n",
    "                                    if len(menuprices) != 0:\n",
    "                                        menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                                    menu_lst.append((menuname, menuprice))\n",
    "                            else:\n",
    "                                for menu in phototype:\n",
    "                                    menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                                    menuprices = menu.select('.info_menu > .price_menu')\n",
    "                                    menuprice = ''\n",
    "\n",
    "                                    if len(menuprices) != 0:\n",
    "                                        menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                                    menu_lst.append((menuname, menuprice))\n",
    "\n",
    "                            if place_hour == \"\":\n",
    "                                try:\n",
    "                                    place_hour = soup.select('.location_present > ul > li > span')[0].text\n",
    "                                except:\n",
    "                                    place_hour = \"\"\n",
    "\n",
    "                            place_photo = \"\"\n",
    "                            try:\n",
    "                                photo = driver.find_element_by_css_selector('span.bg_present')\n",
    "                                photo_url = photo.get_attribute('style')\n",
    "                                m = re.search('\"(.+?)\"', photo_url)\n",
    "                                if m:\n",
    "                                    place_photo = m.group(1)\n",
    "                                else:\n",
    "                                    place_photo = \"\"\n",
    "                            except:\n",
    "                                place_photo = \"\"\n",
    "\n",
    "                            try:\n",
    "                                page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
    "                            except:\n",
    "                                page_num = 0\n",
    "\n",
    "                            stars = []\n",
    "                            reviews = []\n",
    "                            names = []\n",
    "                            nicknames = []\n",
    "                            review_lists = soup.select('.list_evaluation > li')\n",
    "\n",
    "                            if page_num == 0:\n",
    "                                if len(review_lists) != 0:\n",
    "                                    t1 = driver.page_source\n",
    "                                    t2 = BeautifulSoup(t1, \"html.parser\")\n",
    "                                    t3 = t2.find(name=\"div\", attrs={\"class\":\"evaluation_review\"})\n",
    "                            #         print(t3)\n",
    "\n",
    "                                    star = t3.find_all('em',{'class':'num_rate'})\n",
    "                                    review = t3.find_all('p',{'class':'txt_comment'})\n",
    "                                    name = t3.find_all('a', {'class': 'link_user'})\n",
    "\n",
    "                                    stars.extend(star)\n",
    "                                    reviews.extend(review)\n",
    "                                    nicknames.extend(name)\n",
    "                                else:\n",
    "                                    pass\n",
    "                            else:\n",
    "                                for i in range(1, page_num+1):\n",
    "                                    sleep(1)\n",
    "                                    t1 = driver.page_source\n",
    "                                    t2 = BeautifulSoup(t1, \"html.parser\")\n",
    "                                    t3 = t2.find(name=\"div\", attrs={\"class\":\"evaluation_review\"})\n",
    "                            #         print(t3)\n",
    "\n",
    "                                    star = t3.find_all('em',{'class':'num_rate'})\n",
    "                                    review = t3.find_all('p',{'class':'txt_comment'})\n",
    "                                    name = t3.find_all('a', {'class': 'link_user'})\n",
    "\n",
    "                            #         names.append(place)\n",
    "                                    stars.extend(star)\n",
    "                                    reviews.extend(review)\n",
    "                                    nicknames.extend(name)\n",
    "\n",
    "                                    i += 1\n",
    "\n",
    "                                    if i > page_num:\n",
    "                                        break\n",
    "\n",
    "                                    next_page = driver.find_element_by_xpath(\"//a[@data-page='\" + str(i) + \"']\")\n",
    "                                    next_page.send_keys(Keys.ENTER)\n",
    "                                    sleep(1)\n",
    "\n",
    "                            driver.close()\n",
    "                            driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                            df = df.append({\n",
    "                                'name': place_name,\n",
    "                                'category': place_category,\n",
    "                                'address' : place_address,\n",
    "                                'tel': place_tel,\n",
    "                                'time': place_hour,\n",
    "                                'menu': menu_lst,\n",
    "                                'image': place_photo\n",
    "                            }, ignore_index=True)\n",
    "\n",
    "                            names += [place_name] * len(stars)\n",
    "                            for name, user_id, star, review in zip(names, nicknames, stars, reviews):\n",
    "                                row = [name, user_id['data-userid'], star.text[0], review.find(name=\"span\").text]\n",
    "                                series = pd.Series(row, index=df_review.columns)\n",
    "                                df_review = df_review.append(series, ignore_index=True)\n",
    "\n",
    "                            print(place_name, len(df))\n",
    "\n",
    "\n",
    "                    next_btn = driver.find_element_by_id(\"info.search.page.next\")\n",
    "                    has_next = \"disabled\" not in next_btn.get_attribute(\"class\").split(\" \")\n",
    "                    if not has_next:\n",
    "                        print('Arrow is Disabled')\n",
    "                    else:\n",
    "                        Page += 1\n",
    "                        next_btn.send_keys(Keys.ENTER)\n",
    "            # 5페이지가 안넘을 때\n",
    "            else:\n",
    "                for i in range(1, 6):\n",
    "                    xPath = '//*[@id=\"info.search.page.no' + str(i) + '\"]'\n",
    "                    try:\n",
    "                        page = driver.find_element_by_xpath(xPath)\n",
    "                        page.send_keys(Keys.ENTER)\n",
    "                    except ElementNotInteractableException:\n",
    "                        print('End of Page')\n",
    "                        break;\n",
    "                    time.sleep(random.randint(1, 10))\n",
    "                    place_lists = driver.find_elements_by_css_selector('#info\\.search\\.place\\.list > li')\n",
    "                    for p in place_lists: # WebElement\n",
    "                        store_html = p.get_attribute('innerHTML')\n",
    "                        store_info = BeautifulSoup(store_html, \"html.parser\")\n",
    "                        place_name = store_info.select('.head_item > .tit_name > .link_name')\n",
    "                        if len(place_name) == 0:\n",
    "                            continue # 광고\n",
    "\n",
    "                        place_name = store_info.select('.head_item > .tit_name > .link_name')[0].text\n",
    "                        place_address = store_info.select('.info_item > .addr > p')[0].text\n",
    "                        place_hour = store_info.select('.info_item > .openhour > p > a')[0].text\n",
    "                        place_tel = store_info.select('.info_item > .contact > span')[0].text\n",
    "                        place_category = store_info.select('.head_item > span')[0].text\n",
    "\n",
    "                        time.sleep(random.randint(1, 10))\n",
    "                        detail = p.find_element_by_css_selector('div.info_item > div.contact > a.moreview')\n",
    "                        detail.send_keys(Keys.ENTER)\n",
    "\n",
    "                        driver.switch_to.window(driver.window_handles[-1])\n",
    "                        driver.implicitly_wait(1) # 기다려 주자\n",
    "\n",
    "                        menu_lst = []\n",
    "\n",
    "                        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                        while True:\n",
    "                            # 스크롤 끝까지 내리기\n",
    "                            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                            sleep(2)\n",
    "                            # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
    "                            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                            if new_height == last_height:\n",
    "                                break\n",
    "                            last_height = new_height\n",
    "\n",
    "                        html = driver.page_source\n",
    "                        soup = BeautifulSoup(html, 'html.parser')\n",
    "                        menu_all = soup.select('.cont_menu > .list_menu > .nophoto_type')\n",
    "                        menu_only_all = soup.select('.cont_menu > .list_menu > .menuonly_type')\n",
    "                        phototype = soup.select('.cont_menu > .list_menu > .photo_type')\n",
    "    #                     print(menu_all)\n",
    "                        if len(menu_all) != 0:\n",
    "                            for menu in menu_all:\n",
    "                                menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                                menuprices = menu.select('.info_menu > .price_menu')\n",
    "                                menuprice = ''\n",
    "\n",
    "                                if len(menuprices) != 0:\n",
    "                                    menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                                menu_lst.append((menuname, menuprice))\n",
    "                        elif len(menu_only_all) != 0:\n",
    "                            for menu in menu_only_all:\n",
    "                                menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                                menuprices = menu.select('.info_menu > .price_menu')\n",
    "                                menuprice = ''\n",
    "\n",
    "                                if len(menuprices) != 0:\n",
    "                                    menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                                menu_lst.append((menuname, menuprice))\n",
    "                        else:\n",
    "                            for menu in phototype:\n",
    "                                menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                                menuprices = menu.select('.info_menu > .price_menu')\n",
    "                                menuprice = ''\n",
    "\n",
    "                                if len(menuprices) != 0:\n",
    "                                    menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                                menu_lst.append((menuname, menuprice))\n",
    "\n",
    "                        if place_hour == \"\":\n",
    "                            try:\n",
    "                                place_hour = soup.select('.location_present > ul > li > span')[0].text\n",
    "                            except:\n",
    "                                place_hour = \"\"\n",
    "                        \n",
    "                        sleep(1)\n",
    "\n",
    "                        place_photo = \"\"\n",
    "                        try:\n",
    "                            photo = driver.find_element_by_css_selector('span.bg_present')\n",
    "                            photo_url = photo.get_attribute('style')\n",
    "                            m = re.search('\"(.+?)\"', photo_url)\n",
    "                            if m:\n",
    "                                place_photo = m.group(1)\n",
    "                            else:\n",
    "                                place_photo = \"\"\n",
    "                        except:\n",
    "                            place_photo = \"\"\n",
    "\n",
    "                        try:\n",
    "                            page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
    "                        except:\n",
    "                            page_num = 0\n",
    "\n",
    "                        stars = []\n",
    "                        reviews = []\n",
    "                        names = []\n",
    "                        nicknames = []\n",
    "                        review_lists = soup.select('.list_evaluation > li')\n",
    "\n",
    "                        if page_num == 0:\n",
    "                            if len(review_lists) != 0:\n",
    "                                t1 = driver.page_source\n",
    "                                t2 = BeautifulSoup(t1, \"html.parser\")\n",
    "                                t3 = t2.find(name=\"div\", attrs={\"class\":\"evaluation_review\"})\n",
    "                        #         print(t3)\n",
    "\n",
    "                                star = t3.find_all('em',{'class':'num_rate'})\n",
    "                                review = t3.find_all('p',{'class':'txt_comment'})\n",
    "                                name = t3.find_all('a', {'class': 'link_user'})\n",
    "\n",
    "                                stars.extend(star)\n",
    "                                reviews.extend(review)\n",
    "                                nicknames.extend(name)\n",
    "                            else:\n",
    "                                pass\n",
    "                        else:\n",
    "                            for i in range(1, page_num+1):\n",
    "                                t1 = driver.page_source\n",
    "                                t2 = BeautifulSoup(t1, \"html.parser\")\n",
    "                                t3 = t2.find(name=\"div\", attrs={\"class\":\"evaluation_review\"})\n",
    "                        #         print(t3)\n",
    "\n",
    "                                star = t3.find_all('em',{'class':'num_rate'})\n",
    "                                review = t3.find_all('p',{'class':'txt_comment'})\n",
    "                                name = t3.find_all('a', {'class': 'link_user'})\n",
    "\n",
    "                        #         names.append(place)\n",
    "                                stars.extend(star)\n",
    "                                reviews.extend(review)\n",
    "                                nicknames.extend(name)\n",
    "\n",
    "                                i += 1\n",
    "\n",
    "                                if i > page_num:\n",
    "                                    break\n",
    "\n",
    "                                next_page = driver.find_element_by_xpath(\"//a[@data-page='\" + str(i) + \"']\")\n",
    "                                next_page.send_keys(Keys.ENTER)\n",
    "                                sleep(1)\n",
    "\n",
    "                        driver.close()\n",
    "                        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                        df = df.append({\n",
    "                            'name': place_name,\n",
    "                            'category': place_category,\n",
    "                            'address' : place_address,\n",
    "                            'tel': place_tel,\n",
    "                            'time': place_hour,\n",
    "                            'menu': menu_lst,\n",
    "                            'image': place_photo\n",
    "                        }, ignore_index=True)\n",
    "\n",
    "                        names += [place_name] * len(stars)\n",
    "                        for name, user_id, star, review in zip(names, nicknames, stars, reviews):\n",
    "                            row = [name, user_id['data-userid'], star.text[0], review.find(name=\"span\").text]\n",
    "                            series = pd.Series(row, index=df_review.columns)\n",
    "                            df_review = df_review.append(series, ignore_index=True)\n",
    "\n",
    "                        print(place_name, len(df))\n",
    "                \n",
    "        # 못누르는 경우\n",
    "        elif len(more_page.get_attribute(\"class\").split(' ')) > 1:\n",
    "            place_lists = driver.find_elements_by_css_selector('#info\\.search\\.place\\.list > li')\n",
    "            for p in place_lists: # WebElement\n",
    "                store_html = p.get_attribute('innerHTML')\n",
    "                store_info = BeautifulSoup(store_html, \"html.parser\")\n",
    "                place_name = store_info.select('.head_item > .tit_name > .link_name')\n",
    "                if len(place_name) == 0:\n",
    "                    continue # 광고\n",
    "\n",
    "                place_name = store_info.select('.head_item > .tit_name > .link_name')[0].text\n",
    "                place_address = store_info.select('.info_item > .addr > p')[0].text\n",
    "                place_hour = store_info.select('.info_item > .openhour > p > a')[0].text\n",
    "                place_tel = store_info.select('.info_item > .contact > span')[0].text\n",
    "                place_category = store_info.select('.head_item > span')[0].text\n",
    "\n",
    "                time.sleep(random.randint(1, 10))\n",
    "                detail = p.find_element_by_css_selector('div.info_item > div.contact > a.moreview')\n",
    "                detail.send_keys(Keys.ENTER)\n",
    "\n",
    "                driver.switch_to.window(driver.window_handles[-1])\n",
    "                driver.implicitly_wait(1) # 기다려 주자\n",
    "\n",
    "                menu_lst = []\n",
    "\n",
    "                last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                while True:\n",
    "                    # 스크롤 끝까지 내리기\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                    sleep(2)\n",
    "                    # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
    "                    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "                    if new_height == last_height:\n",
    "                        break\n",
    "                    last_height = new_height\n",
    "\n",
    "                html = driver.page_source\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                menu_all = soup.select('.cont_menu > .list_menu > .nophoto_type')\n",
    "                menu_only_all = soup.select('.cont_menu > .list_menu > .menuonly_type')\n",
    "                phototype = soup.select('.cont_menu > .list_menu > .photo_type')\n",
    "#                     print(menu_all)\n",
    "                if len(menu_all) != 0:\n",
    "                    for menu in menu_all:\n",
    "                        menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                        menuprices = menu.select('.info_menu > .price_menu')\n",
    "                        menuprice = ''\n",
    "\n",
    "                        if len(menuprices) != 0:\n",
    "                            menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                        menu_lst.append((menuname, menuprice))\n",
    "                elif len(menu_only_all) != 0:\n",
    "                    for menu in menu_only_all:\n",
    "                        menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                        menuprices = menu.select('.info_menu > .price_menu')\n",
    "                        menuprice = ''\n",
    "\n",
    "                        if len(menuprices) != 0:\n",
    "                            menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                        menu_lst.append((menuname, menuprice))\n",
    "                else:\n",
    "                    for menu in phototype:\n",
    "                        menuname = menu.select('.info_menu > .loss_word')[0].text\n",
    "                        menuprices = menu.select('.info_menu > .price_menu')\n",
    "                        menuprice = ''\n",
    "\n",
    "                        if len(menuprices) != 0:\n",
    "                            menuprice = menuprices[0].text.split(' ')[1]\n",
    "\n",
    "                        menu_lst.append((menuname, menuprice))\n",
    "                        \n",
    "                if place_hour == \"\":\n",
    "                    try:\n",
    "                        place_hour = soup.select('.location_present > ul > li > span')[0].text\n",
    "                    except:\n",
    "                        place_hour = \"\"\n",
    "\n",
    "                place_photo = \"\"\n",
    "                try:\n",
    "                    photo = driver.find_element_by_css_selector('span.bg_present')\n",
    "                    photo_url = photo.get_attribute('style')\n",
    "                    m = re.search('\"(.+?)\"', photo_url)\n",
    "                    if m:\n",
    "                        place_photo = m.group(1)\n",
    "                    else:\n",
    "                        place_photo = \"\"\n",
    "                except:\n",
    "                    place_photo = \"\"\n",
    "\n",
    "                try:\n",
    "                    page_num = len(driver.find_elements_by_class_name('link_page')) # 페이지 수 찾기\n",
    "                except:\n",
    "                    page_num = 0\n",
    "\n",
    "                stars = []\n",
    "                reviews = []\n",
    "                names = []\n",
    "                nicknames = []\n",
    "                review_lists = soup.select('.list_evaluation > li')\n",
    "\n",
    "                if page_num == 0:\n",
    "                    if len(review_lists) != 0:\n",
    "                        t1 = driver.page_source\n",
    "                        t2 = BeautifulSoup(t1, \"html.parser\")\n",
    "                        t3 = t2.find(name=\"div\", attrs={\"class\":\"evaluation_review\"})\n",
    "                #         print(t3)\n",
    "\n",
    "                        star = t3.find_all('em',{'class':'num_rate'})\n",
    "                        review = t3.find_all('p',{'class':'txt_comment'})\n",
    "                        name = t3.find_all('a', {'class': 'link_user'})\n",
    "\n",
    "                        stars.extend(star)\n",
    "                        reviews.extend(review)\n",
    "                        nicknames.extend(name)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    for i in range(1, page_num+1):\n",
    "                        t1 = driver.page_source\n",
    "                        t2 = BeautifulSoup(t1, \"html.parser\")\n",
    "                        t3 = t2.find(name=\"div\", attrs={\"class\":\"evaluation_review\"})\n",
    "                #         print(t3)\n",
    "\n",
    "                        star = t3.find_all('em',{'class':'num_rate'})\n",
    "                        review = t3.find_all('p',{'class':'txt_comment'})\n",
    "                        name = t3.find_all('a', {'class': 'link_user'})\n",
    "\n",
    "                #         names.append(place)\n",
    "                        stars.extend(star)\n",
    "                        reviews.extend(review)\n",
    "                        nicknames.extend(name)\n",
    "\n",
    "                        i += 1\n",
    "\n",
    "                        if i > page_num:\n",
    "                            break\n",
    "\n",
    "                        next_page = driver.find_element_by_xpath(\"//a[@data-page='\" + str(i) + \"']\")\n",
    "                        next_page.send_keys(Keys.ENTER)\n",
    "                        sleep(1)\n",
    "\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "                df = df.append({\n",
    "                    'name': place_name,\n",
    "                    'category': place_category,\n",
    "                    'address' : place_address,\n",
    "                    'tel': place_tel,\n",
    "                    'time': place_hour,\n",
    "                    'menu': menu_lst,\n",
    "                    'image': place_photo\n",
    "                }, ignore_index=True)\n",
    "\n",
    "                names += [place_name] * len(stars)\n",
    "                for name, user_id, star, review in zip(names, nicknames, stars, reviews):\n",
    "                    row = [name, user_id['data-userid'], star.text[0], review.find(name=\"span\").text]\n",
    "                    series = pd.Series(row, index=df_review.columns)\n",
    "                    df_review = df_review.append(series, ignore_index=True)\n",
    "\n",
    "                print(place_name, len(df))\n",
    "                \n",
    "        driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9598c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path_or_buf='kakao_map_ver5.csv', encoding='utf-8-sig')\n",
    "df_review.to_csv(path_or_buf='kakao_review_ver5.csv', encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
