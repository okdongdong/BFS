{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08569696",
   "metadata": {},
   "source": [
    "# 공공데이터 포털 관광지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcccbef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "pages = 6\n",
    "API_key = ''\n",
    "\n",
    "name_list = []\n",
    "gugun_list = []\n",
    "lat_list = []\n",
    "lng_list = []\n",
    "place_list = []\n",
    "add_list = []\n",
    "tel_list = []\n",
    "home_list = []\n",
    "holiday_list = []\n",
    "image_list = []\n",
    "sub_list = []\n",
    "search_list = []\n",
    "\n",
    "# for page in range(1, 2):\n",
    "for page in range(1, pages+1):\n",
    "    url = f'http://apis.data.go.kr/6260000/AttractionService/getAttractionKr?serviceKey={API_key}&pageNo={page}&numOfRows=20&resultType=json'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    n_dict = json.loads(response.text)\n",
    "\n",
    "#     print(n_dict['getAttractionKr'])\n",
    "    for area_name in n_dict['getAttractionKr']['item']:\n",
    "        name_list.append(area_name['MAIN_TITLE'])\n",
    "        gugun_list.append(area_name['GUGUN_NM'])\n",
    "        lat_list.append(area_name['LAT'])\n",
    "        lng_list.append(area_name['LNG'])\n",
    "        if area_name['PLACE'].count('(') > 0:\n",
    "            x = area_name['PLACE'].split('(')[0]\n",
    "            place_list.append(x)\n",
    "        else:\n",
    "            place_list.append(area_name['PLACE'])\n",
    "        add_list.append(area_name['ADDR1'])\n",
    "        tel_list.append(area_name['CNTCT_TEL'])\n",
    "        home_list.append(area_name['HOMEPAGE_URL'])\n",
    "        holiday_list.append(area_name['HLDY_INFO'])\n",
    "        image_list.append(area_name['MAIN_IMG_NORMAL'])\n",
    "        sub_list.append(area_name['ITEMCNTNTS'])\n",
    "        if area_name['PLACE'].count('(') > 0:\n",
    "            x = \"부산 \" + area_name['PLACE'].split('(')[0]\n",
    "            search_list.append(x)\n",
    "        else:\n",
    "            search_list.append(\"부산 \" + area_name['PLACE'])\n",
    "        print(area_name['MAIN_TITLE'], \"완료\")\n",
    "        \n",
    "    sleep(0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e9204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['name','GUN', 'address', 'lat', 'lng', 'tel', 'hompage', 'holiday', 'image', 'explain' ]\n",
    "df = pd.DataFrame(columns=columns)\n",
    "for place, gun, address, lat, lng, tel, home, holiday, image, explain in zip(place_list, gugun_list, add_list, lat_list, lng_list, tel_list, home_list, holiday_list, image_list, sub_list):\n",
    "    row = [place, gun, address, lat, lng, tel, home, holiday, image, explain]\n",
    "    series = pd.Series(row, index=df.columns)\n",
    "    df = df.append(series, ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c712c27",
   "metadata": {},
   "source": [
    "# 카카오맵 리뷰 크롤링"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4cf4c",
   "metadata": {},
   "source": [
    "### 라이브러리 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import ElementNotVisibleException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "# 경고 메시지를 무시하고 숨기거나\n",
    "warnings.filterwarnings(action='ignore')\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('headless')\n",
    "options.add_argument('lang=ko_KR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0e7e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(place):\n",
    "    global driver\n",
    "    \n",
    "    check = False\n",
    "    search_area = driver.find_element_by_xpath('//*[@id=\"search.keyword.query\"]')\n",
    "    search_area.send_keys(place)\n",
    "    driver.find_element_by_xpath('//*[@id=\"search.keyword.submit\"]').send_keys(Keys.ENTER)\n",
    "    \n",
    "    try:\n",
    "        driver.find_element_by_xpath('//*[@id=\"info.search.place.list\"]/li[1]')\n",
    "    except:\n",
    "        return\n",
    "    \n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    review = (soup.select('#info\\.search\\.place\\.list > li.PlaceItem.clickArea > div.rating.clickArea > span.score > a'))\n",
    "#     print(x[0]['href'])\n",
    "#     review = soup.select('#info\\.search\\.place\\.list > li.PlaceItem.clickArea.PlaceItem-ACTIVE > div.rating.clickArea > span.score > a')\n",
    "    review_link = review[0]['href']\n",
    "    \n",
    "    # 새탭 열기\n",
    "    driver.execute_script('window.open(\"about:blank\", \"_blank\");')\n",
    "    tabs = driver.window_handles\n",
    "    driver.switch_to.window(tabs[1]) # 탭 이동\n",
    "    driver.get(review_link)\n",
    "    \n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # 스크롤 끝까지 내리기\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        sleep(2)\n",
    "        # 스크롤 다운 후 스크롤 높이 다시 가져옴\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "    \n",
    "        tmp = driver.page_source\n",
    "        tmp2 = BeautifulSoup(tmp, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            review_find = tmp2.select('#mArticle > div.cont_evaluation > strong > span')\n",
    "            total_page = math.ceil(int(review_find[0].text) / 5)\n",
    "        except:\n",
    "            check = True\n",
    "            break\n",
    "#         print(total_review)\n",
    "    \n",
    "    if check == True:\n",
    "        pass\n",
    "    else:\n",
    "        for i in range(1, total_page+1):\n",
    "            t1 = driver.page_source\n",
    "            t2 = BeautifulSoup(t1, \"html.parser\")\n",
    "            t3 = t2.find(name=\"div\", attrs={\"class\":\"evaluation_review\"})\n",
    "    #         print(t3)\n",
    "\n",
    "            star = t3.find_all('em',{'class':'num_rate'})\n",
    "            review = t3.find_all('p',{'class':'txt_comment'})\n",
    "            name = t3.find_all('a', {'class': 'link_user'})\n",
    "\n",
    "    #         names.append(place)\n",
    "            stars.extend(star)\n",
    "            reviews.extend(review)\n",
    "            nicknames.extend(name)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "            if i > total_page:\n",
    "                break\n",
    "\n",
    "            next_page = driver.find_element_by_xpath(\"//a[@data-page='\" + str(i) + \"']\")\n",
    "            next_page.send_keys(Keys.ENTER)\n",
    "            sleep(1)\n",
    "    \n",
    "    \n",
    "stars = []\n",
    "reviews = []\n",
    "names = []\n",
    "nicknames = []\n",
    "    \n",
    "# driver.implicitly_wait(4) \n",
    "# driver.get('https://map.kakao.com/')\n",
    "\n",
    "# search('스포원파크')\n",
    "for tour_name in search_list:\n",
    "# for tour_name in ['부산 아난티코브 타운, 이터널저니']:\n",
    "    driver = webdriver.Chrome('./chromedriver')\n",
    "    driver.maximize_window()\n",
    "    driver.implicitly_wait(4) \n",
    "    driver.get('https://map.kakao.com/')\n",
    "    search(tour_name)\n",
    "    names += [tour_name] * len(stars)\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d76cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['name', 'user_id', 'score', 'review']\n",
    "df_review = pd.DataFrame(columns=columns)\n",
    "for name, user_id, star, review in zip(names, nicknames, stars, reviews):\n",
    "    row = [name, user_id['data-userid'], star.text[0], review.find(name=\"span\").text]\n",
    "    series = pd.Series(row, index=df_review.columns)\n",
    "    df_review = df_review.append(series, ignore_index=True)\n",
    "\n",
    "print(df_review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
